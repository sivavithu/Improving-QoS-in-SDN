{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sivavithu/Improving-QoS-in-SDN/blob/main/RF6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXwztfwZa38v",
        "outputId": "fbb36b16-559a-43e7-fbee-11b610a0af10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_VKmBjLcMrg",
        "outputId": "acd5aeed-853a-4012-ce53-3f6ff173934a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Enhanced CIC-Darknet2020 Application Classifier\n",
            "Target: 95%+ Accuracy for SDN Application Classification\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Import all required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "\n",
        "print(\"üöÄ Enhanced CIC-Darknet2020 Application Classifier\")\n",
        "print(\"Target: 95%+ Accuracy for SDN Application Classification\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqRBmCiicjHw",
        "outputId": "bfdded72-76fe-402c-895e-efecf5f23dff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ All ML libraries imported!\n"
          ]
        }
      ],
      "source": [
        "# ===============================================================================\n",
        "# CELL 3: Import ML Libraries\n",
        "# ===============================================================================\n",
        "\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, VotingClassifier, StackingClassifier\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler, LabelEncoder\n",
        "from sklearn.feature_selection import SelectKBest, f_classif, RFE, RFECV\n",
        "from sklearn.metrics import (classification_report, confusion_matrix, accuracy_score,\n",
        "                           precision_recall_fscore_support)\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.combine import SMOTETomek\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "import joblib\n",
        "\n",
        "print(\"‚úÖ All ML libraries imported!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 620
        },
        "id": "mjR3eImlctNz",
        "outputId": "7b8f2d48-a4f8-42a1-d339-d24b7469680f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Dataset Shape: (158616, 85)\n",
            "üìä Memory Usage: 155.32 MB\n",
            "\n",
            "üìã Dataset Info:\n",
            "  Rows: 158,616\n",
            "  Columns: 85\n",
            "  Missing values: 48\n",
            "\n",
            "üìù First 5 rows:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                      Flow ID         Src IP  Src Port  \\\n",
              "0     10.152.152.11-216.58.220.99-57158-443-6  10.152.152.11     57158   \n",
              "1     10.152.152.11-216.58.220.99-57159-443-6  10.152.152.11     57159   \n",
              "2     10.152.152.11-216.58.220.99-57160-443-6  10.152.152.11     57160   \n",
              "3    10.152.152.11-74.125.136.120-49134-443-6  10.152.152.11     49134   \n",
              "4  10.152.152.11-173.194.65.127-34697-19305-6  10.152.152.11     34697   \n",
              "\n",
              "           Dst IP  Dst Port  Protocol               Timestamp  Flow Duration  \\\n",
              "0   216.58.220.99       443         6  24/07/2015 04:09:48 PM            229   \n",
              "1   216.58.220.99       443         6  24/07/2015 04:09:48 PM            407   \n",
              "2   216.58.220.99       443         6  24/07/2015 04:09:48 PM            431   \n",
              "3  74.125.136.120       443         6  24/07/2015 04:09:48 PM            359   \n",
              "4  173.194.65.127     19305         6  24/07/2015 04:09:45 PM       10778451   \n",
              "\n",
              "   Total Fwd Packet  Total Bwd packets  ...  Active Mean  Active Std  \\\n",
              "0                 1                  1  ...            0           0   \n",
              "1                 1                  1  ...            0           0   \n",
              "2                 1                  1  ...            0           0   \n",
              "3                 1                  1  ...            0           0   \n",
              "4               591                400  ...            0           0   \n",
              "\n",
              "   Active Max  Active Min     Idle Mean      Idle Std      Idle Max  \\\n",
              "0           0           0  0.000000e+00  0.000000e+00  0.000000e+00   \n",
              "1           0           0  0.000000e+00  0.000000e+00  0.000000e+00   \n",
              "2           0           0  0.000000e+00  0.000000e+00  0.000000e+00   \n",
              "3           0           0  0.000000e+00  0.000000e+00  0.000000e+00   \n",
              "4           0           0  1.437765e+15  3.117718e+06  1.437765e+15   \n",
              "\n",
              "       Idle Min    Label          Label.1  \n",
              "0  0.000000e+00  Non-Tor  AUDIO-STREAMING  \n",
              "1  0.000000e+00  Non-Tor  AUDIO-STREAMING  \n",
              "2  0.000000e+00  Non-Tor  AUDIO-STREAMING  \n",
              "3  0.000000e+00  Non-Tor  AUDIO-STREAMING  \n",
              "4  1.437765e+15  Non-Tor  AUDIO-STREAMING  \n",
              "\n",
              "[5 rows x 85 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-86fed0a5-9a15-454b-9795-1ab756ac7bc0\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Flow ID</th>\n",
              "      <th>Src IP</th>\n",
              "      <th>Src Port</th>\n",
              "      <th>Dst IP</th>\n",
              "      <th>Dst Port</th>\n",
              "      <th>Protocol</th>\n",
              "      <th>Timestamp</th>\n",
              "      <th>Flow Duration</th>\n",
              "      <th>Total Fwd Packet</th>\n",
              "      <th>Total Bwd packets</th>\n",
              "      <th>...</th>\n",
              "      <th>Active Mean</th>\n",
              "      <th>Active Std</th>\n",
              "      <th>Active Max</th>\n",
              "      <th>Active Min</th>\n",
              "      <th>Idle Mean</th>\n",
              "      <th>Idle Std</th>\n",
              "      <th>Idle Max</th>\n",
              "      <th>Idle Min</th>\n",
              "      <th>Label</th>\n",
              "      <th>Label.1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10.152.152.11-216.58.220.99-57158-443-6</td>\n",
              "      <td>10.152.152.11</td>\n",
              "      <td>57158</td>\n",
              "      <td>216.58.220.99</td>\n",
              "      <td>443</td>\n",
              "      <td>6</td>\n",
              "      <td>24/07/2015 04:09:48 PM</td>\n",
              "      <td>229</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>Non-Tor</td>\n",
              "      <td>AUDIO-STREAMING</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10.152.152.11-216.58.220.99-57159-443-6</td>\n",
              "      <td>10.152.152.11</td>\n",
              "      <td>57159</td>\n",
              "      <td>216.58.220.99</td>\n",
              "      <td>443</td>\n",
              "      <td>6</td>\n",
              "      <td>24/07/2015 04:09:48 PM</td>\n",
              "      <td>407</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>Non-Tor</td>\n",
              "      <td>AUDIO-STREAMING</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10.152.152.11-216.58.220.99-57160-443-6</td>\n",
              "      <td>10.152.152.11</td>\n",
              "      <td>57160</td>\n",
              "      <td>216.58.220.99</td>\n",
              "      <td>443</td>\n",
              "      <td>6</td>\n",
              "      <td>24/07/2015 04:09:48 PM</td>\n",
              "      <td>431</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>Non-Tor</td>\n",
              "      <td>AUDIO-STREAMING</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10.152.152.11-74.125.136.120-49134-443-6</td>\n",
              "      <td>10.152.152.11</td>\n",
              "      <td>49134</td>\n",
              "      <td>74.125.136.120</td>\n",
              "      <td>443</td>\n",
              "      <td>6</td>\n",
              "      <td>24/07/2015 04:09:48 PM</td>\n",
              "      <td>359</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>Non-Tor</td>\n",
              "      <td>AUDIO-STREAMING</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10.152.152.11-173.194.65.127-34697-19305-6</td>\n",
              "      <td>10.152.152.11</td>\n",
              "      <td>34697</td>\n",
              "      <td>173.194.65.127</td>\n",
              "      <td>19305</td>\n",
              "      <td>6</td>\n",
              "      <td>24/07/2015 04:09:45 PM</td>\n",
              "      <td>10778451</td>\n",
              "      <td>591</td>\n",
              "      <td>400</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.437765e+15</td>\n",
              "      <td>3.117718e+06</td>\n",
              "      <td>1.437765e+15</td>\n",
              "      <td>1.437765e+15</td>\n",
              "      <td>Non-Tor</td>\n",
              "      <td>AUDIO-STREAMING</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows √ó 85 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-86fed0a5-9a15-454b-9795-1ab756ac7bc0')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-86fed0a5-9a15-454b-9795-1ab756ac7bc0 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-86fed0a5-9a15-454b-9795-1ab756ac7bc0');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-063c1548-a821-4b21-b6c8-c67cab2fec35\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-063c1548-a821-4b21-b6c8-c67cab2fec35')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-063c1548-a821-4b21-b6c8-c67cab2fec35 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# ===============================================================================\n",
        "# CELL 4: Load and Inspect Dataset\n",
        "# ===============================================================================\n",
        "\n",
        "# Load your dataset - UPDATE THIS PATH\n",
        "df = pd.read_csv('/content/drive/MyDrive/Research/Darknet.CSV')\n",
        "\n",
        "print(f\"üìä Dataset Shape: {df.shape}\")\n",
        "print(f\"üìä Memory Usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "\n",
        "# Display basic info\n",
        "print(\"\\nüìã Dataset Info:\")\n",
        "print(f\"  Rows: {len(df):,}\")\n",
        "print(f\"  Columns: {len(df.columns)}\")\n",
        "print(f\"  Missing values: {df.isnull().sum().sum():,}\")\n",
        "\n",
        "# Show first few rows\n",
        "print(\"\\nüìù First 5 rows:\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AN9vxhX2c9on",
        "outputId": "ef454e5d-fe25-4221-8bdd-a926ac505504"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç SEARCHING FOR APPLICATION COLUMN\n",
            "========================================\n",
            "\n",
            "üìä Column: 'Flow ID' (79160 unique values)\n",
            "   Sample values: ['10.152.152.11-216.58.220.99-57158-443-6', '10.152.152.11-216.58.220.99-57159-443-6', '10.152.152.11-216.58.220.99-57160-443-6', '10.152.152.11-74.125.136.120-49134-443-6', '10.152.152.11-173.194.65.127-34697-19305-6']\n",
            "   ‚ùå Unlikely application column (score: 0)\n",
            "\n",
            "üìä Column: 'Src IP' (4026 unique values)\n",
            "   Sample values: ['10.152.152.11', '173.194.33.97', '74.125.28.189', '74.125.228.199', '173.194.65.100']\n",
            "   ‚ùå Unlikely application column (score: 0)\n",
            "\n",
            "üìä Column: 'Dst IP' (7553 unique values)\n",
            "   Sample values: ['216.58.220.99', '74.125.136.120', '173.194.65.127', '10.152.152.11', '216.58.216.142']\n",
            "   ‚ùå Unlikely application column (score: 0)\n",
            "\n",
            "üìä Column: 'Timestamp' (34836 unique values)\n",
            "   Sample values: ['24/07/2015 04:09:48 PM', '24/07/2015 04:09:45 PM', '24/07/2015 04:10:00 PM', '24/07/2015 04:09:46 PM', '24/07/2015 04:09:49 PM']\n",
            "   ‚ùå Unlikely application column (score: 0)\n",
            "\n",
            "üìä Column: 'Label' (4 unique values)\n",
            "   Sample values: ['Non-Tor', 'NonVPN', 'Tor', 'VPN']\n",
            "   ‚ùå Unlikely application column (score: 0)\n",
            "\n",
            "üìä Column: 'Label.1' (11 unique values)\n",
            "   Sample values: ['AUDIO-STREAMING', 'Chat', 'Audio-Streaming', 'Browsing', 'Email']\n",
            "   ‚úÖ STRONG application candidate (score: 10)\n",
            "\n",
            "üéØ SELECTED: 'Label.1' (score: 10)\n",
            "\n",
            "‚úÖ Using column: 'Label.1'\n"
          ]
        }
      ],
      "source": [
        "# ===============================================================================\n",
        "# CELL 5: Find Application Column\n",
        "# ===============================================================================\n",
        "\n",
        "def find_application_column_enhanced(df):\n",
        "    \"\"\"Find the correct application classification column\"\"\"\n",
        "\n",
        "    print(\"üîç SEARCHING FOR APPLICATION COLUMN\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "    # Check all object columns\n",
        "    object_columns = df.select_dtypes(include=['object']).columns\n",
        "\n",
        "    app_candidates = []\n",
        "\n",
        "    for col in object_columns:\n",
        "        unique_vals = df[col].unique()\n",
        "        sample_text = ' '.join([str(v).lower() for v in unique_vals[:10]])\n",
        "\n",
        "        print(f\"\\nüìä Column: '{col}' ({len(unique_vals)} unique values)\")\n",
        "        print(f\"   Sample values: {list(unique_vals[:5])}\")\n",
        "\n",
        "        # Enhanced application detection\n",
        "        app_keywords = ['browsing', 'chat', 'email', 'audio', 'video', 'voip',\n",
        "                       'p2p', 'stream', 'web', 'http', 'ftp', 'torrent']\n",
        "\n",
        "        keyword_matches = sum(1 for keyword in app_keywords if keyword in sample_text)\n",
        "\n",
        "        # Check for application patterns\n",
        "        has_streaming = 'stream' in sample_text\n",
        "        has_protocols = any(p in sample_text for p in ['http', 'ftp', 'smtp'])\n",
        "        reasonable_classes = 5 <= len(unique_vals) <= 25\n",
        "\n",
        "        score = keyword_matches\n",
        "        if has_streaming: score += 2\n",
        "        if has_protocols: score += 1\n",
        "        if reasonable_classes: score += 1\n",
        "\n",
        "        app_candidates.append((col, score, len(unique_vals)))\n",
        "\n",
        "        if score >= 3:\n",
        "            print(f\"   ‚úÖ STRONG application candidate (score: {score})\")\n",
        "        elif score >= 1:\n",
        "            print(f\"   ‚ö†Ô∏è Possible application column (score: {score})\")\n",
        "        else:\n",
        "            print(f\"   ‚ùå Unlikely application column (score: {score})\")\n",
        "\n",
        "    # Sort by score\n",
        "    app_candidates.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    if app_candidates and app_candidates[0][1] > 0:\n",
        "        best_col = app_candidates[0][0]\n",
        "        print(f\"\\nüéØ SELECTED: '{best_col}' (score: {app_candidates[0][1]})\")\n",
        "        return best_col\n",
        "    else:\n",
        "        print(\"\\n‚ö†Ô∏è No clear application column found!\")\n",
        "        return object_columns[0] if len(object_columns) > 0 else df.columns[-1]\n",
        "\n",
        "# Find application column\n",
        "app_column = find_application_column_enhanced(df)\n",
        "print(f\"\\n‚úÖ Using column: '{app_column}'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================================================\n",
        "# INSERT THIS CODE AFTER CELL 5 (Find Application Column)\n",
        "# ===============================================================================\n",
        "\n",
        "def normalize_application_names(df, app_column):\n",
        "    \"\"\"Normalize application class names to handle case inconsistencies\"\"\"\n",
        "\n",
        "    print(\"üîß NORMALIZING APPLICATION CLASS NAMES\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "    # Get original distribution\n",
        "    original_dist = df[app_column].value_counts()\n",
        "    print(f\"üìä Original classes: {len(original_dist)}\")\n",
        "\n",
        "    # Show original distribution\n",
        "    print(\"\\nüìã Original Distribution:\")\n",
        "    for app, count in original_dist.items():\n",
        "        print(f\"  {app:<30} {count:6,}\")\n",
        "\n",
        "    # Normalize class names\n",
        "    df_normalized = df.copy()\n",
        "\n",
        "    # Convert to lowercase and standardize\n",
        "    df_normalized[app_column] = df_normalized[app_column].str.lower()\n",
        "\n",
        "    # Apply consistent formatting rules\n",
        "    normalization_rules = {\n",
        "        'audio-streaming': 'Audio-Streaming',\n",
        "        'video-streaming': 'Video-Streaming',\n",
        "        'file-transfer': 'File-Transfer',\n",
        "        'browsing': 'Browsing',\n",
        "        'chat': 'Chat',\n",
        "        'email': 'Email',\n",
        "        'p2p': 'P2P',\n",
        "        'voip': 'VOIP'\n",
        "    }\n",
        "\n",
        "    # Apply normalization\n",
        "    for old_name, new_name in normalization_rules.items():\n",
        "        df_normalized[app_column] = df_normalized[app_column].replace(old_name, new_name)\n",
        "\n",
        "    # Get normalized distribution\n",
        "    normalized_dist = df_normalized[app_column].value_counts()\n",
        "    print(f\"\\n‚úÖ Normalized classes: {len(normalized_dist)}\")\n",
        "\n",
        "    # Show normalized distribution\n",
        "    print(\"\\nüìã Normalized Distribution:\")\n",
        "    total_samples = len(df_normalized)\n",
        "    for app, count in normalized_dist.items():\n",
        "        percentage = count / total_samples * 100\n",
        "        print(f\"  {app:<30} {count:6,} ({percentage:5.1f}%)\")\n",
        "\n",
        "    # Show the consolidation effects\n",
        "    print(f\"\\nüéØ CONSOLIDATION EFFECTS:\")\n",
        "    original_classes = set(original_dist.index)\n",
        "    normalized_classes = set(normalized_dist.index)\n",
        "\n",
        "    # Find merged classes\n",
        "    for norm_class in normalized_classes:\n",
        "        # Find all original classes that map to this normalized class\n",
        "        original_variants = []\n",
        "        for orig_class in original_classes:\n",
        "            if orig_class.lower().replace('-', '-') == norm_class.lower().replace('-', '-'):\n",
        "                original_variants.append(orig_class)\n",
        "\n",
        "        if len(original_variants) > 1:\n",
        "            total_count = sum(original_dist[variant] for variant in original_variants)\n",
        "            print(f\"  üìé {norm_class}: merged {original_variants} ‚Üí {total_count:,} total samples\")\n",
        "\n",
        "    classes_reduced = len(original_dist) - len(normalized_dist)\n",
        "    if classes_reduced > 0:\n",
        "        print(f\"\\n‚ú® Reduced classes by {classes_reduced} (from {len(original_dist)} to {len(normalized_dist)})\")\n",
        "\n",
        "    return df_normalized\n",
        "\n",
        "# Apply normalization\n",
        "print(f\"\\nüîß APPLYING CLASS NAME NORMALIZATION\")\n",
        "print(\"=\" * 42)\n",
        "\n",
        "df = normalize_application_names(df, app_column)\n",
        "\n",
        "print(f\"‚úÖ Class normalization completed!\")\n",
        "print(f\"üìä Final dataset shape: {df.shape}\")\n",
        "\n",
        "# Verify the fix worked\n",
        "final_dist = df[app_column].value_counts()\n",
        "print(f\"\\nüéØ FINAL CLASS VERIFICATION:\")\n",
        "print(\"=\" * 30)\n",
        "for i, (app, count) in enumerate(final_dist.items()):\n",
        "    percentage = count / len(df) * 100\n",
        "    print(f\"  {i+1:2d}. {app:<25} {count:6,} ({percentage:5.1f}%)\")\n",
        "\n",
        "# Check for any remaining case issues\n",
        "class_names_lower = [name.lower() for name in final_dist.index]\n",
        "if len(class_names_lower) != len(set(class_names_lower)):\n",
        "    print(\"‚ö†Ô∏è WARNING: Still have case inconsistencies!\")\n",
        "else:\n",
        "    print(\"‚úÖ All class names are now properly normalized!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkTieRKnifXu",
        "outputId": "d826c92f-8da7-40fe-d652-0f199a2d971e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîß APPLYING CLASS NAME NORMALIZATION\n",
            "==========================================\n",
            "üîß NORMALIZING APPLICATION CLASS NAMES\n",
            "========================================\n",
            "üìä Original classes: 11\n",
            "\n",
            "üìã Original Distribution:\n",
            "  P2P                            48,520\n",
            "  Browsing                       46,457\n",
            "  Audio-Streaming                19,830\n",
            "  Chat                           11,629\n",
            "  File-Transfer                  11,098\n",
            "  Video-Streaming                 9,486\n",
            "  Email                           6,145\n",
            "  VOIP                            3,566\n",
            "  AUDIO-STREAMING                 1,520\n",
            "  Video-streaming                   281\n",
            "  File-transfer                      84\n",
            "\n",
            "‚úÖ Normalized classes: 8\n",
            "\n",
            "üìã Normalized Distribution:\n",
            "  P2P                            48,520 ( 30.6%)\n",
            "  Browsing                       46,457 ( 29.3%)\n",
            "  Audio-Streaming                21,350 ( 13.5%)\n",
            "  Chat                           11,629 (  7.3%)\n",
            "  File-Transfer                  11,182 (  7.0%)\n",
            "  Video-Streaming                 9,767 (  6.2%)\n",
            "  Email                           6,145 (  3.9%)\n",
            "  VOIP                            3,566 (  2.2%)\n",
            "\n",
            "üéØ CONSOLIDATION EFFECTS:\n",
            "  üìé File-Transfer: merged ['File-Transfer', 'File-transfer'] ‚Üí 11,182 total samples\n",
            "  üìé Audio-Streaming: merged ['AUDIO-STREAMING', 'Audio-Streaming'] ‚Üí 21,350 total samples\n",
            "  üìé Video-Streaming: merged ['Video-streaming', 'Video-Streaming'] ‚Üí 9,767 total samples\n",
            "\n",
            "‚ú® Reduced classes by 3 (from 11 to 8)\n",
            "‚úÖ Class normalization completed!\n",
            "üìä Final dataset shape: (158616, 85)\n",
            "\n",
            "üéØ FINAL CLASS VERIFICATION:\n",
            "==============================\n",
            "   1. P2P                       48,520 ( 30.6%)\n",
            "   2. Browsing                  46,457 ( 29.3%)\n",
            "   3. Audio-Streaming           21,350 ( 13.5%)\n",
            "   4. Chat                      11,629 (  7.3%)\n",
            "   5. File-Transfer             11,182 (  7.0%)\n",
            "   6. Video-Streaming            9,767 (  6.2%)\n",
            "   7. Email                      6,145 (  3.9%)\n",
            "   8. VOIP                       3,566 (  2.2%)\n",
            "‚úÖ All class names are now properly normalized!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHb1y0ZIdDyL",
        "outputId": "d67e2346-2551-4d17-b2b8-2aba2c2dee83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üéØ APPLICATION ANALYSIS\n",
            "=========================\n",
            "Application Column: 'Label.1'\n",
            "Number of Applications: 8\n",
            "\n",
            "üìä Application Distribution:\n",
            "   1. P2P                            48,520 ( 30.6%)\n",
            "   2. Browsing                       46,457 ( 29.3%)\n",
            "   3. Audio-Streaming                21,350 ( 13.5%)\n",
            "   4. Chat                           11,629 (  7.3%)\n",
            "   5. File-Transfer                  11,182 (  7.0%)\n",
            "   6. Video-Streaming                 9,767 (  6.2%)\n",
            "   7. Email                           6,145 (  3.9%)\n",
            "   8. VOIP                            3,566 (  2.2%)\n",
            "‚úÖ Good for classification: 8 classes\n"
          ]
        }
      ],
      "source": [
        "# ===============================================================================\n",
        "# CELL 6: Analyze Application Distribution\n",
        "# ===============================================================================\n",
        "\n",
        "# Analyze the selected application column\n",
        "y_raw = df[app_column].copy()\n",
        "app_distribution = y_raw.value_counts()\n",
        "\n",
        "print(f\"üéØ APPLICATION ANALYSIS\")\n",
        "print(\"=\" * 25)\n",
        "print(f\"Application Column: '{app_column}'\")\n",
        "print(f\"Number of Applications: {len(app_distribution)}\")\n",
        "\n",
        "print(f\"\\nüìä Application Distribution:\")\n",
        "for i, (app, count) in enumerate(app_distribution.items()):\n",
        "    percentage = count / len(df) * 100\n",
        "    print(f\"  {i+1:2d}. {app:<30} {count:6,} ({percentage:5.1f}%)\")\n",
        "\n",
        "# Check if suitable for classification\n",
        "if len(app_distribution) < 2:\n",
        "    print(\"‚ùå ERROR: Need at least 2 classes for classification!\")\n",
        "elif len(app_distribution) > 50:\n",
        "    print(\"‚ö†Ô∏è WARNING: Too many classes, might affect performance\")\n",
        "else:\n",
        "    print(f\"‚úÖ Good for classification: {len(app_distribution)} classes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QbZgcWsodSND",
        "outputId": "f2c6d4cf-5894-47b0-84b5-400c51325d10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß ENHANCED DATA PREPROCESSING\n",
            "===================================\n",
            "Initial: 84 features, 158616 samples\n",
            "üóëÔ∏è Removed SDN-incompatible: ['Flow ID', 'Src IP', 'Dst IP', 'Src Port', 'Dst Port', 'Timestamp']\n",
            "üóëÔ∏è Removed label column: Label\n",
            "üîÑ Handling infinite values...\n",
            "üóëÔ∏è Removed 15 constant columns\n",
            "üìÑ Removed 56244 duplicates\n",
            "‚úÖ Final: 62 features, 102372 samples\n",
            "üè∑Ô∏è LABEL ENCODING RESULTS\n",
            "============================\n",
            "Number of classes: 8\n",
            "\n",
            "üìä Encoded Class Distribution:\n",
            "   0. Audio-Streaming           13,493 samples ( 13.2%)\n",
            "   1. Browsing                  29,803 samples ( 29.1%)\n",
            "   2. Chat                       9,654 samples (  9.4%)\n",
            "   3. Email                      5,313 samples (  5.2%)\n",
            "   4. File-Transfer             10,474 samples ( 10.2%)\n",
            "   5. P2P                       23,233 samples ( 22.7%)\n",
            "   6. VOIP                       1,930 samples (  1.9%)\n",
            "   7. Video-Streaming            8,472 samples (  8.3%)\n",
            "\n",
            "üìà Class imbalance ratio: 15.44\n",
            "‚ö†Ô∏è HIGH imbalance - will apply advanced balancing\n"
          ]
        }
      ],
      "source": [
        "# ===============================================================================\n",
        "# CELL 7: Data Preprocessing\n",
        "# ===============================================================================\n",
        "\n",
        "def preprocess_data_enhanced(df, target_col):\n",
        "    \"\"\"Enhanced data preprocessing for maximum accuracy\"\"\"\n",
        "\n",
        "    print(\"üîß ENHANCED DATA PREPROCESSING\")\n",
        "    print(\"=\" * 35)\n",
        "\n",
        "    # Separate features and target\n",
        "    X = df.drop(columns=[target_col]).copy()\n",
        "    y = df[target_col].copy()\n",
        "\n",
        "    print(f\"Initial: {X.shape[1]} features, {len(X)} samples\")\n",
        "\n",
        "    # Remove ID and network-specific columns (not available in SDN)\n",
        "    sdn_incompatible = [\n",
        "        'Flow ID', 'Src IP', 'Dst IP', 'Src Port', 'Dst Port', 'Timestamp',\n",
        "        'flow_id', 'src_ip', 'dst_ip', 'src_port', 'dst_port', 'timestamp',\n",
        "        'FlowID', 'SrcIP', 'DstIP', 'SrcPort', 'DstPort', 'Flow_ID'\n",
        "    ]\n",
        "\n",
        "    removed_cols = []\n",
        "    for col in sdn_incompatible:\n",
        "        if col in X.columns:\n",
        "            X = X.drop(columns=[col])\n",
        "            removed_cols.append(col)\n",
        "\n",
        "    if removed_cols:\n",
        "        print(f\"üóëÔ∏è Removed SDN-incompatible: {removed_cols}\")\n",
        "\n",
        "    # Remove other label columns\n",
        "    other_labels = ['Label', 'label', 'class', 'Class', 'Attack', 'attack']\n",
        "    for col in list(X.columns):\n",
        "        if any(label in col for label in other_labels):\n",
        "            X = X.drop(columns=[col])\n",
        "            print(f\"üóëÔ∏è Removed label column: {col}\")\n",
        "\n",
        "    # Convert object columns to numeric\n",
        "    for col in X.columns:\n",
        "        if X[col].dtype == 'object':\n",
        "            try:\n",
        "                X[col] = pd.to_numeric(X[col], errors='coerce')\n",
        "            except:\n",
        "                le = LabelEncoder()\n",
        "                X[col] = le.fit_transform(X[col].astype(str))\n",
        "                print(f\"üî§ Label encoded: {col}\")\n",
        "\n",
        "    # Handle infinite values\n",
        "    print(\"üîÑ Handling infinite values...\")\n",
        "    X = X.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "    # Remove high missing columns (>85% missing)\n",
        "    missing_thresh = 0.85\n",
        "    high_missing = X.columns[X.isnull().sum() / len(X) > missing_thresh]\n",
        "    if len(high_missing) > 0:\n",
        "        X = X.drop(columns=high_missing)\n",
        "        print(f\"üóëÔ∏è Removed {len(high_missing)} high-missing columns\")\n",
        "\n",
        "    # Remove constant columns\n",
        "    constant_cols = []\n",
        "    for col in X.columns:\n",
        "        if X[col].nunique() <= 1:\n",
        "            constant_cols.append(col)\n",
        "\n",
        "    if constant_cols:\n",
        "        X = X.drop(columns=constant_cols)\n",
        "        print(f\"üóëÔ∏è Removed {len(constant_cols)} constant columns\")\n",
        "\n",
        "    # Smart missing value imputation\n",
        "    for col in X.columns:\n",
        "        if X[col].isnull().sum() > 0:\n",
        "            if abs(X[col].skew()) > 1:  # Skewed data\n",
        "                X[col] = X[col].fillna(X[col].median())\n",
        "            else:  # Normal data\n",
        "                X[col] = X[col].fillna(X[col].mean())\n",
        "\n",
        "    # Remove duplicates\n",
        "    before_dedup = len(X)\n",
        "    duplicated = X.duplicated()\n",
        "    X = X[~duplicated]\n",
        "    y = y[~duplicated]\n",
        "    after_dedup = len(X)\n",
        "\n",
        "    if before_dedup != after_dedup:\n",
        "        print(f\"üìÑ Removed {before_dedup - after_dedup} duplicates\")\n",
        "\n",
        "    print(f\"‚úÖ Final: {X.shape[1]} features, {len(X)} samples\")\n",
        "\n",
        "    return X, y\n",
        "\n",
        "# Apply preprocessing\n",
        "X_clean, y_clean = preprocess_data_enhanced(df, app_column)\n",
        "\n",
        "# ===============================================================================\n",
        "# CELL 8: Label Encoding and Analysis\n",
        "# ===============================================================================\n",
        "\n",
        "# Encode target labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y_clean)\n",
        "class_names = label_encoder.classes_\n",
        "\n",
        "print(\"üè∑Ô∏è LABEL ENCODING RESULTS\")\n",
        "print(\"=\" * 28)\n",
        "print(f\"Number of classes: {len(class_names)}\")\n",
        "\n",
        "# Show class distribution\n",
        "unique_classes, class_counts = np.unique(y_encoded, return_counts=True)\n",
        "print(f\"\\nüìä Encoded Class Distribution:\")\n",
        "for i, (cls, count) in enumerate(zip(class_names, class_counts)):\n",
        "    print(f\"  {i:2d}. {cls:<25} {count:6,} samples ({count/len(y_encoded)*100:5.1f}%)\")\n",
        "\n",
        "# Calculate class imbalance\n",
        "imbalance_ratio = max(class_counts) / min(class_counts)\n",
        "print(f\"\\nüìà Class imbalance ratio: {imbalance_ratio:.2f}\")\n",
        "\n",
        "if imbalance_ratio > 10:\n",
        "    print(\"‚ö†Ô∏è HIGH imbalance - will apply advanced balancing\")\n",
        "elif imbalance_ratio > 3:\n",
        "    print(\"‚ö†Ô∏è MODERATE imbalance - will apply balancing\")\n",
        "else:\n",
        "    print(\"‚úÖ Classes reasonably balanced\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-Nnf80sd37m",
        "outputId": "037efbc5-6b67-4c55-b918-9734bdf1d713"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚öôÔ∏è FEATURE ENGINEERING\n",
            "=======================\n",
            "Creating statistical features...\n",
            "Creating ratio features from 32 flow columns...\n",
            "‚úÖ Added 11 engineered features\n",
            "üìä Total features: 73\n"
          ]
        }
      ],
      "source": [
        "# ===============================================================================\n",
        "# CELL 9: Feature Engineering\n",
        "# ===============================================================================\n",
        "\n",
        "def create_advanced_features(X):\n",
        "    \"\"\"Create engineered features for better classification\"\"\"\n",
        "\n",
        "    print(\"‚öôÔ∏è FEATURE ENGINEERING\")\n",
        "    print(\"=\" * 23)\n",
        "\n",
        "    X_eng = X.copy()\n",
        "    initial_count = X_eng.shape[1]\n",
        "\n",
        "    # Statistical features across all features for each sample\n",
        "    print(\"Creating statistical features...\")\n",
        "    X_eng['row_sum'] = X_eng.sum(axis=1)\n",
        "    X_eng['row_mean'] = X_eng.mean(axis=1)\n",
        "    X_eng['row_std'] = X_eng.std(axis=1)\n",
        "    X_eng['row_max'] = X_eng.max(axis=1)\n",
        "    X_eng['row_min'] = X_eng.min(axis=1)\n",
        "    X_eng['row_range'] = X_eng['row_max'] - X_eng['row_min']\n",
        "    X_eng['row_median'] = X_eng.median(axis=1)\n",
        "\n",
        "    # Network flow specific features\n",
        "    flow_keywords = ['byte', 'packet', 'length', 'duration', 'rate', 'time']\n",
        "    flow_cols = [col for col in X.columns if any(kw in col.lower() for kw in flow_keywords)]\n",
        "\n",
        "    if len(flow_cols) >= 4:\n",
        "        print(f\"Creating ratio features from {len(flow_cols)} flow columns...\")\n",
        "\n",
        "        # Create meaningful ratios\n",
        "        for i in range(0, min(len(flow_cols), 8), 2):\n",
        "            if i+1 < len(flow_cols):\n",
        "                col1, col2 = flow_cols[i], flow_cols[i+1]\n",
        "                ratio_name = f'ratio_{i//2}'\n",
        "                X_eng[ratio_name] = X_eng[col1] / (X_eng[col2] + 1e-8)\n",
        "\n",
        "                # Handle infinite ratios\n",
        "                X_eng[ratio_name] = X_eng[ratio_name].replace([np.inf, -np.inf], 0)\n",
        "\n",
        "    # Remove any columns that became constant after engineering\n",
        "    for col in X_eng.columns:\n",
        "        if X_eng[col].nunique() <= 1:\n",
        "            X_eng = X_eng.drop(columns=[col])\n",
        "\n",
        "    new_features = X_eng.shape[1] - initial_count\n",
        "    print(f\"‚úÖ Added {new_features} engineered features\")\n",
        "    print(f\"üìä Total features: {X_eng.shape[1]}\")\n",
        "\n",
        "    return X_eng\n",
        "\n",
        "# Apply feature engineering\n",
        "X_engineered = create_advanced_features(X_clean)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFA6lL-5d5d6",
        "outputId": "5d3b16e4-9bfa-4680-f63d-6856bfa191fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üéØ ADVANCED FEATURE SELECTION\n",
            "Target: 80 features\n",
            "================================\n",
            "Stage 1: Correlation filtering...\n",
            "  Removed 17 highly correlated features\n",
            "Stage 2: Univariate selection...\n",
            "  Selected 56 features\n",
            "Stage 3: Recursive elimination...\n",
            "  RFECV selected 51 optimal features\n",
            "Stage 4: Importance-based selection...\n",
            "‚úÖ Final selection: 51 features\n",
            "\n",
            "üèÜ Top 10 Selected Features:\n",
            "   1. Idle Min                  (0.0912)\n",
            "   2. Idle Mean                 (0.0854)\n",
            "   3. FWD Init Win Bytes        (0.0454)\n",
            "   4. Flow IAT Max              (0.0404)\n",
            "   5. Flow IAT Min              (0.0394)\n",
            "   6. Flow Duration             (0.0372)\n",
            "   7. Bwd Init Win Bytes        (0.0358)\n",
            "   8. Fwd Packets/s             (0.0327)\n",
            "   9. Flow Packets/s            (0.0310)\n",
            "  10. ratio_0                   (0.0305)\n",
            "\n",
            "‚úÖ Feature selection completed!\n",
            "üìä Selected 51 features from 73 original features\n"
          ]
        }
      ],
      "source": [
        "# ===============================================================================\n",
        "# CELL 10: Advanced Feature Selection\n",
        "# ===============================================================================\n",
        "\n",
        "def select_best_features(X, y, target_features=80):\n",
        "    \"\"\"Multi-stage feature selection for optimal performance\"\"\"\n",
        "\n",
        "    print(f\"üéØ ADVANCED FEATURE SELECTION\")\n",
        "    print(f\"Target: {target_features} features\")\n",
        "    print(\"=\" * 32)\n",
        "\n",
        "    # Stage 1: Remove highly correlated features\n",
        "    print(\"Stage 1: Correlation filtering...\")\n",
        "    corr_matrix = X.corr().abs()\n",
        "\n",
        "    # Find highly correlated pairs\n",
        "    upper_triangle = corr_matrix.where(\n",
        "        np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
        "    )\n",
        "\n",
        "    high_corr_features = [column for column in upper_triangle.columns\n",
        "                         if any(upper_triangle[column] > 0.95)]\n",
        "\n",
        "    if high_corr_features:\n",
        "        X = X.drop(columns=high_corr_features)\n",
        "        print(f\"  Removed {len(high_corr_features)} highly correlated features\")\n",
        "\n",
        "    # Stage 2: Univariate feature selection\n",
        "    print(\"Stage 2: Univariate selection...\")\n",
        "    k_univariate = min(target_features * 2, X.shape[1])\n",
        "\n",
        "    selector_univariate = SelectKBest(score_func=f_classif, k=k_univariate)\n",
        "    X_univariate = selector_univariate.fit_transform(X, y)\n",
        "    selected_features = X.columns[selector_univariate.get_support()]\n",
        "\n",
        "    print(f\"  Selected {len(selected_features)} features\")\n",
        "\n",
        "    # Stage 3: Recursive Feature Elimination\n",
        "    print(\"Stage 3: Recursive elimination...\")\n",
        "\n",
        "    # Use a fast estimator for RFE\n",
        "    rf_estimator = RandomForestClassifier(\n",
        "        n_estimators=50,\n",
        "        random_state=RANDOM_STATE,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    # RFECV with cross-validation\n",
        "    rfecv = RFECV(\n",
        "        estimator=rf_estimator,\n",
        "        step=1,\n",
        "        cv=3,\n",
        "        scoring='accuracy',\n",
        "        min_features_to_select=max(20, target_features//2),\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    X_rfe = rfecv.fit_transform(X_univariate, y)\n",
        "    final_features = selected_features[rfecv.get_support()]\n",
        "\n",
        "    print(f\"  RFECV selected {len(final_features)} optimal features\")\n",
        "\n",
        "    # Stage 4: Final selection based on importance\n",
        "    print(\"Stage 4: Importance-based selection...\")\n",
        "\n",
        "    rf_final = RandomForestClassifier(\n",
        "        n_estimators=100,\n",
        "        random_state=RANDOM_STATE,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    rf_final.fit(X_rfe, y)\n",
        "\n",
        "    # Get feature importances\n",
        "    importances = rf_final.feature_importances_\n",
        "\n",
        "    # Select top features\n",
        "    n_final = min(target_features, len(final_features))\n",
        "    top_indices = np.argsort(importances)[-n_final:]\n",
        "\n",
        "    X_final = pd.DataFrame(\n",
        "        X_rfe[:, top_indices],\n",
        "        columns=final_features[top_indices]\n",
        "    )\n",
        "\n",
        "    # Create feature importance dataframe\n",
        "    feature_importance_df = pd.DataFrame({\n",
        "        'feature': final_features[top_indices],\n",
        "        'importance': importances[top_indices]\n",
        "    }).sort_values('importance', ascending=False)\n",
        "\n",
        "    print(f\"‚úÖ Final selection: {n_final} features\")\n",
        "\n",
        "    print(f\"\\nüèÜ Top 10 Selected Features:\")\n",
        "    for i, (_, row) in enumerate(feature_importance_df.head(10).iterrows()):\n",
        "        print(f\"  {i+1:2d}. {row['feature']:<25} ({row['importance']:.4f})\")\n",
        "\n",
        "    return X_final, final_features[top_indices], feature_importance_df\n",
        "# ===============================================================================\n",
        "# CELL 15: Create Advanced Ensembles\n",
        "# ===============================================================================\n",
        "\n",
        "print(\"üîó CREATING ADVANCED ENSEMBLES\")\n",
        "print(\"=\" * 34)\n",
        "\n",
        "# Voting Classifier (Soft Voting)\n",
        "print(\"Creating Voting Ensemble...\")\n",
        "voting_classifier = VotingClassifier(\n",
        "‚Ä¶print(f\"\\nüèÜ ENSEMBLE RESULTS:\")\n",
        "print(\"=\" * 20)\n",
        "for name in ['Voting_Ensemble', 'Stacking_Ensemble']:\n",
        "    score = validation_scores[name]\n",
        "    status = \"üéØ\" if score >= 0.95 else \"üìà\" if score >= 0.90 else \"‚ö†Ô∏è\"\n",
        "    print(f\"  {status} {name:<18} {score:.4f}\")\n",
        "\n",
        "# Apply feature selection\n",
        "X_selected, selected_feature_names, feature_importance = select_best_features(\n",
        "    X_engineered, y_encoded, target_features=80\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Feature selection completed!\")\n",
        "print(f\"üìä Selected {X_selected.shape[1]} features from {X_engineered.shape[1]} original features\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDIfMBMYeUks",
        "outputId": "4116cef0-cde0-4c4e-d6f6-7f7e619dcbdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä DATA SPLITTING AND SCALING\n",
            "================================\n",
            "üìà Training set:   71,701 samples (70.0%)\n",
            "üìà Validation set: 15,315 samples (15.0%)\n",
            "üìà Test set:       15,356 samples (15.0%)\n",
            "\n",
            "‚öñÔ∏è Applying RobustScaler...\n",
            "‚úÖ Scaling completed!\n",
            "\n",
            "üîç Data validation:\n",
            "  Training features shape: (71701, 51)\n",
            "  Validation features shape: (15315, 51)\n",
            "  Test features shape: (15356, 51)\n",
            "  No missing values: True\n"
          ]
        }
      ],
      "source": [
        "# ===============================================================================\n",
        "# CELL 11: Data Splitting and Scaling\n",
        "# ===============================================================================\n",
        "\n",
        "print(\"üìä DATA SPLITTING AND SCALING\")\n",
        "print(\"=\" * 32)\n",
        "\n",
        "# Stratified train-test split\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "    X_selected, y_encoded,\n",
        "    test_size=0.15,\n",
        "    random_state=RANDOM_STATE,\n",
        "    stratify=y_encoded\n",
        ")\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_temp, y_temp,\n",
        "    test_size=0.176,  # This gives us 70-15-15 split\n",
        "    random_state=RANDOM_STATE,\n",
        "    stratify=y_temp\n",
        ")\n",
        "\n",
        "print(f\"üìà Training set:   {len(X_train):,} samples ({len(X_train)/len(X_selected)*100:.1f}%)\")\n",
        "print(f\"üìà Validation set: {len(X_val):,} samples ({len(X_val)/len(X_selected)*100:.1f}%)\")\n",
        "print(f\"üìà Test set:       {len(X_test):,} samples ({len(X_test)/len(X_selected)*100:.1f}%)\")\n",
        "\n",
        "# Advanced scaling with RobustScaler (handles outliers better)\n",
        "print(\"\\n‚öñÔ∏è Applying RobustScaler...\")\n",
        "scaler = RobustScaler()\n",
        "\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"‚úÖ Scaling completed!\")\n",
        "\n",
        "# Verify no data leakage\n",
        "print(f\"\\nüîç Data validation:\")\n",
        "print(f\"  Training features shape: {X_train_scaled.shape}\")\n",
        "print(f\"  Validation features shape: {X_val_scaled.shape}\")\n",
        "print(f\"  Test features shape: {X_test_scaled.shape}\")\n",
        "print(f\"  No missing values: {not np.isnan(X_train_scaled).any()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ta1X5bDy_Cn",
        "outputId": "fbeb1039-cfff-482e-d8f8-2e6fe63b429b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚öñÔ∏è ADVANCED CLASS BALANCING\n",
            "==============================\n",
            "üìä Current training distribution:\n",
            "  Audio-Streaming           9,450 samples (13.2%)\n",
            "  Browsing                  20,874 samples (29.1%)\n",
            "  Chat                      6,762 samples (9.4%)\n",
            "  Email                     3,721 samples (5.2%)\n",
            "  File-Transfer             7,336 samples (10.2%)\n",
            "  P2P                       16,272 samples (22.7%)\n",
            "  VOIP                      1,352 samples (1.9%)\n",
            "  Video-Streaming           5,934 samples (8.3%)\n",
            "\n",
            "üìà Imbalance ratio: 15.44\n",
            "üîÑ Applying SMOTE + Tomek Links for optimal balancing...\n",
            "üìä After SMOTE + Tomek:\n",
            "  Audio-Streaming           19,936 samples\n",
            "  Browsing                  19,591 samples\n",
            "  Chat                      18,891 samples\n",
            "  Email                     18,626 samples\n",
            "  File-Transfer             19,761 samples\n",
            "  P2P                       20,336 samples\n",
            "  VOIP                      18,969 samples\n",
            "  Video-Streaming           19,664 samples\n",
            "‚úÖ Balanced dataset: 155,774 samples\n",
            "ü§ñ CREATING OPTIMIZED MODELS\n",
            "==============================\n",
            "Creating Enhanced Random Forest...\n",
            "Creating Extra Trees Classifier...\n",
            "Creating XGBoost Classifier...\n",
            "Creating LightGBM Classifier...\n",
            "‚úÖ Created 4 optimized models\n"
          ]
        }
      ],
      "source": [
        "# ===============================================================================\n",
        "# CELL 12: Class Balancing\n",
        "# ===============================================================================\n",
        "\n",
        "def apply_advanced_balancing(X_train, y_train):\n",
        "    \"\"\"Apply advanced class balancing techniques\"\"\"\n",
        "\n",
        "    print(\"‚öñÔ∏è ADVANCED CLASS BALANCING\")\n",
        "    print(\"=\" * 30)\n",
        "\n",
        "    # Analyze current distribution\n",
        "    unique_train, counts_train = np.unique(y_train, return_counts=True)\n",
        "\n",
        "    print(\"üìä Current training distribution:\")\n",
        "    for cls, count in zip(unique_train, counts_train):\n",
        "        cls_name = class_names[cls]\n",
        "        print(f\"  {cls_name:<25} {count:,} samples ({count/len(y_train)*100:.1f}%)\")\n",
        "\n",
        "    imbalance_ratio = max(counts_train) / min(counts_train)\n",
        "    print(f\"\\nüìà Imbalance ratio: {imbalance_ratio:.2f}\")\n",
        "\n",
        "    if imbalance_ratio > 5:\n",
        "        print(\"üîÑ Applying SMOTE + Tomek Links for optimal balancing...\")\n",
        "\n",
        "        try:\n",
        "            # SMOTETomek combines oversampling and undersampling\n",
        "            smote_tomek = SMOTETomek(\n",
        "                smote=SMOTE(random_state=RANDOM_STATE, k_neighbors=3),\n",
        "                random_state=RANDOM_STATE\n",
        "            )\n",
        "\n",
        "            X_balanced, y_balanced = smote_tomek.fit_resample(X_train, y_train)\n",
        "\n",
        "            print(\"üìä After SMOTE + Tomek:\")\n",
        "            unique_balanced, counts_balanced = np.unique(y_balanced, return_counts=True)\n",
        "            for cls, count in zip(unique_balanced, counts_balanced):\n",
        "                cls_name = class_names[cls]\n",
        "                print(f\"  {cls_name:<25} {count:,} samples\")\n",
        "\n",
        "            print(f\"‚úÖ Balanced dataset: {len(X_balanced):,} samples\")\n",
        "            return X_balanced, y_balanced\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Balancing failed: {e}\")\n",
        "            print(\"üìä Using original data with class weights in models\")\n",
        "            return X_train, y_train\n",
        "\n",
        "    elif imbalance_ratio > 2:\n",
        "        print(\"üîÑ Applying standard SMOTE...\")\n",
        "\n",
        "        try:\n",
        "            smote = SMOTE(random_state=RANDOM_STATE, k_neighbors=3)\n",
        "            X_balanced, y_balanced = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "            print(f\"‚úÖ SMOTE applied: {len(X_balanced):,} samples\")\n",
        "            return X_balanced, y_balanced\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è SMOTE failed: {e}\")\n",
        "            return X_train, y_train\n",
        "\n",
        "    else:\n",
        "        print(\"‚úÖ Classes reasonably balanced, no resampling needed\")\n",
        "        return X_train, y_train\n",
        "\n",
        "# Apply balancing\n",
        "X_train_balanced, y_train_balanced = apply_advanced_balancing(X_train_scaled, y_train)\n",
        "\n",
        "# ===============================================================================\n",
        "# CELL 13: Model Definition and Training\n",
        "# ===============================================================================\n",
        "\n",
        "def create_optimized_models():\n",
        "    \"\"\"Create highly optimized models for maximum accuracy\"\"\"\n",
        "\n",
        "    print(\"ü§ñ CREATING OPTIMIZED MODELS\")\n",
        "    print(\"=\" * 30)\n",
        "\n",
        "    models = {}\n",
        "\n",
        "    # 1. Enhanced Random Forest\n",
        "    print(\"Creating Enhanced Random Forest...\")\n",
        "    models['Enhanced_RF'] = RandomForestClassifier(\n",
        "        n_estimators=500,           # More trees\n",
        "        max_depth=25,               # Deeper trees\n",
        "        min_samples_split=2,        # More flexible splits\n",
        "        min_samples_leaf=1,         # More flexible leaves\n",
        "        max_features='sqrt',        # Good for high-dimensional data\n",
        "        bootstrap=True,\n",
        "        class_weight='balanced_subsample',  # Handle imbalance\n",
        "        random_state=RANDOM_STATE,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    # 2. Extra Trees (Extremely Randomized Trees)\n",
        "    print(\"Creating Extra Trees Classifier...\")\n",
        "    models['Extra_Trees'] = ExtraTreesClassifier(\n",
        "        n_estimators=500,\n",
        "        max_depth=30,\n",
        "        min_samples_split=2,\n",
        "        min_samples_leaf=1,\n",
        "        max_features='sqrt',\n",
        "        bootstrap=False,\n",
        "        class_weight='balanced',\n",
        "        random_state=RANDOM_STATE,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    # 3. XGBoost\n",
        "    print(\"Creating XGBoost Classifier...\")\n",
        "    models['XGBoost'] = xgb.XGBClassifier(\n",
        "        n_estimators=500,\n",
        "        max_depth=8,\n",
        "        learning_rate=0.1,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        reg_alpha=0.1,\n",
        "        reg_lambda=0.1,\n",
        "        random_state=RANDOM_STATE,\n",
        "        n_jobs=-1,\n",
        "        eval_metric='mlogloss'\n",
        "    )\n",
        "\n",
        "    # 4. LightGBM\n",
        "    print(\"Creating LightGBM Classifier...\")\n",
        "    models['LightGBM'] = lgb.LGBMClassifier(\n",
        "        n_estimators=500,\n",
        "        max_depth=10,\n",
        "        learning_rate=0.1,\n",
        "        num_leaves=31,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        reg_alpha=0.1,\n",
        "        reg_lambda=0.1,\n",
        "        random_state=RANDOM_STATE,\n",
        "        n_jobs=-1,\n",
        "        verbose=-1\n",
        "    )\n",
        "\n",
        "    print(f\"‚úÖ Created {len(models)} optimized models\")\n",
        "    return models\n",
        "\n",
        "# Create models\n",
        "optimized_models = create_optimized_models()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTW6mmVx1U9h",
        "outputId": "26947169-342c-40bd-d361-de8069909a95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ TRAINING INDIVIDUAL MODELS\n",
            "================================\n",
            "\n",
            "üìö Training Enhanced_RF...\n",
            "   ‚úÖ Enhanced_RF validation accuracy: 0.8739\n",
            "\n",
            "üìö Training Extra_Trees...\n",
            "   ‚úÖ Extra_Trees validation accuracy: 0.8601\n",
            "\n",
            "üìö Training XGBoost...\n",
            "   ‚úÖ XGBoost validation accuracy: 0.8857\n",
            "\n",
            "üìö Training LightGBM...\n",
            "   ‚úÖ LightGBM validation accuracy: 0.8855\n",
            "\n",
            "üìä INDIVIDUAL MODEL RESULTS:\n",
            "===================================\n",
            "  ‚ö†Ô∏è Enhanced_RF     0.8739\n",
            "  ‚ö†Ô∏è Extra_Trees     0.8601\n",
            "  ‚ö†Ô∏è XGBoost         0.8857\n",
            "  ‚ö†Ô∏è LightGBM        0.8855\n"
          ]
        }
      ],
      "source": [
        "# ===============================================================================\n",
        "# CELL 14: Train Individual Models\n",
        "# ===============================================================================\n",
        "\n",
        "print(\"üöÄ TRAINING INDIVIDUAL MODELS\")\n",
        "print(\"=\" * 32)\n",
        "\n",
        "trained_models = {}\n",
        "validation_scores = {}\n",
        "\n",
        "for name, model in optimized_models.items():\n",
        "    print(f\"\\nüìö Training {name}...\")\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train_balanced, y_train_balanced)\n",
        "    trained_models[name] = model\n",
        "\n",
        "    # Validate on validation set\n",
        "    val_pred = model.predict(X_val_scaled)\n",
        "    val_accuracy = accuracy_score(y_val, val_pred)\n",
        "    validation_scores[name] = val_accuracy\n",
        "\n",
        "    print(f\"   ‚úÖ {name} validation accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "    # Check if we've reached our target\n",
        "    if val_accuracy >= 0.95:\n",
        "        print(f\"   üéâ TARGET REACHED: {val_accuracy:.4f} >= 0.95!\")\n",
        "\n",
        "print(f\"\\nüìä INDIVIDUAL MODEL RESULTS:\")\n",
        "print(\"=\" * 35)\n",
        "for name, score in validation_scores.items():\n",
        "    status = \"üéØ\" if score >= 0.95 else \"üìà\" if score >= 0.90 else \"‚ö†Ô∏è\"\n",
        "    print(f\"  {status} {name:<15} {score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Fndsd7Vs1Z0W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6772fc2e-0093-4b22-e099-c786f09b518f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CREATING MEMORY-EFFICIENT ENSEMBLES\n",
            "========================================\n",
            "Memory usage: 71.6% (8.7GB/12.7GB)\n",
            "\n",
            "Selecting best models for ensemble...\n",
            "Model performance ranking:\n",
            "  1. XGBoost: 0.8857\n",
            "  2. LightGBM: 0.8855\n",
            "  3. Enhanced_RF: 0.8739\n",
            "  4. Extra_Trees: 0.8601\n",
            "\n",
            "Selected top 3 models for ensemble: ['XGBoost', 'LightGBM', 'Enhanced_RF']\n",
            "Removed Extra_Trees from memory\n",
            "Memory usage: 71.6% (8.7GB/12.7GB)\n",
            "\n",
            "Creating lightweight Voting Ensemble...\n",
            "Training Voting Ensemble...\n",
            "Voting Ensemble validation accuracy: 0.8859\n",
            "Memory usage: 84.2% (10.3GB/12.7GB)\n",
            "\n",
            "Skipping Stacking Ensemble due to memory constraints...\n",
            "Stacking requires storing predictions from all base models, which exceeds available RAM\n",
            "\n",
            "Creating Memory-Efficient Weighted Ensemble...\n",
            "Model weights: {'XGBoost': np.float64(0.3348309059491484), 'LightGBM': np.float64(0.33478153542335226), 'Enhanced_RF': np.float64(0.33038755862749936)}\n",
            "Weighted Ensemble validation accuracy: 0.8611\n",
            "Memory usage: 84.2% (10.3GB/12.7GB)\n",
            "\n",
            "ENSEMBLE RESULTS:\n",
            "====================\n",
            "  NEEDS IMPROVEMENT: Voting_Ensemble    0.8859\n",
            "  NEEDS IMPROVEMENT: Weighted_Ensemble  0.8611\n",
            "\n",
            "Memory-efficient ensemble creation completed!\n",
            "Available models: ['Enhanced_RF', 'XGBoost', 'LightGBM', 'Voting_Ensemble', 'Weighted_Ensemble']\n"
          ]
        }
      ],
      "source": [
        "# ===============================================================================\n",
        "# CELL 15: Memory-Efficient Advanced Ensembles\n",
        "# ===============================================================================\n",
        "\n",
        "import gc  # For garbage collection\n",
        "\n",
        "print(\"CREATING MEMORY-EFFICIENT ENSEMBLES\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Clear memory before ensemble creation\n",
        "gc.collect()\n",
        "\n",
        "# Memory monitoring function\n",
        "def check_memory_usage():\n",
        "    \"\"\"Check current memory usage\"\"\"\n",
        "    import psutil\n",
        "    memory = psutil.virtual_memory()\n",
        "    print(f\"Memory usage: {memory.percent:.1f}% ({memory.used/1024**3:.1f}GB/{memory.total/1024**3:.1f}GB)\")\n",
        "\n",
        "check_memory_usage()\n",
        "\n",
        "# Strategy 1: Use only the best performing individual models for ensemble\n",
        "print(\"\\nSelecting best models for ensemble...\")\n",
        "\n",
        "# Sort models by validation performance\n",
        "sorted_models = sorted(validation_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "print(\"Model performance ranking:\")\n",
        "for i, (name, score) in enumerate(sorted_models):\n",
        "    print(f\"  {i+1}. {name}: {score:.4f}\")\n",
        "\n",
        "# Select only top 3 models to reduce memory usage\n",
        "top_models = dict(sorted_models[:3])\n",
        "print(f\"\\nSelected top 3 models for ensemble: {list(top_models.keys())}\")\n",
        "\n",
        "# Clear unused models from memory\n",
        "for model_name in list(trained_models.keys()):\n",
        "    if model_name not in top_models:\n",
        "        del trained_models[model_name]\n",
        "        print(f\"Removed {model_name} from memory\")\n",
        "\n",
        "gc.collect()\n",
        "check_memory_usage()\n",
        "\n",
        "# Strategy 2: Lightweight Voting Classifier\n",
        "print(\"\\nCreating lightweight Voting Ensemble...\")\n",
        "\n",
        "try:\n",
        "    # Use hard voting (less memory intensive than soft voting)\n",
        "    voting_classifier = VotingClassifier(\n",
        "        estimators=[(name, trained_models[name]) for name in top_models.keys()],\n",
        "        voting='hard',  # Changed from 'soft' to reduce memory\n",
        "        n_jobs=1        # Reduce parallel processing to save memory\n",
        "    )\n",
        "\n",
        "    print(\"Training Voting Ensemble...\")\n",
        "    voting_classifier.fit(X_train_balanced, y_train_balanced)\n",
        "\n",
        "    # Clear training data temporarily to free memory\n",
        "    X_temp = X_train_balanced\n",
        "    y_temp = y_train_balanced\n",
        "    del X_train_balanced, y_train_balanced\n",
        "    gc.collect()\n",
        "\n",
        "    # Validate voting ensemble\n",
        "    voting_val_pred = voting_classifier.predict(X_val_scaled)\n",
        "    voting_val_accuracy = accuracy_score(y_val, voting_val_pred)\n",
        "    print(f\"Voting Ensemble validation accuracy: {voting_val_accuracy:.4f}\")\n",
        "\n",
        "    # Restore training data\n",
        "    X_train_balanced = X_temp\n",
        "    y_train_balanced = y_temp\n",
        "    del X_temp, y_temp\n",
        "\n",
        "    # Add to models\n",
        "    trained_models['Voting_Ensemble'] = voting_classifier\n",
        "    validation_scores['Voting_Ensemble'] = voting_val_accuracy\n",
        "\n",
        "    if voting_val_accuracy >= 0.95:\n",
        "        print(f\"TARGET REACHED: {voting_val_accuracy:.4f} >= 0.95!\")\n",
        "\n",
        "    gc.collect()\n",
        "    check_memory_usage()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Voting ensemble failed due to memory: {e}\")\n",
        "    print(\"Continuing with individual models only...\")\n",
        "\n",
        "# Strategy 3: Skip Stacking Classifier (most memory intensive)\n",
        "print(\"\\nSkipping Stacking Ensemble due to memory constraints...\")\n",
        "print(\"Stacking requires storing predictions from all base models, which exceeds available RAM\")\n",
        "\n",
        "# Strategy 4: Create a simple weighted ensemble instead\n",
        "print(\"\\nCreating Memory-Efficient Weighted Ensemble...\")\n",
        "\n",
        "class WeightedEnsemble:\n",
        "    def __init__(self, models, weights):\n",
        "        self.models = models\n",
        "        self.weights = weights\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Make weighted predictions\"\"\"\n",
        "        predictions = []\n",
        "\n",
        "        for (name, model), weight in zip(self.models.items(), self.weights):\n",
        "            pred = model.predict(X)\n",
        "            predictions.append(pred * weight)\n",
        "\n",
        "        # Sum weighted predictions and take argmax\n",
        "        weighted_sum = np.sum(predictions, axis=0)\n",
        "        return np.array([np.argmax(np.bincount(row.astype(int))) for row in weighted_sum.reshape(-1, 1)])\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"Make weighted probability predictions if available\"\"\"\n",
        "        if not hasattr(list(self.models.values())[0], 'predict_proba'):\n",
        "            return None\n",
        "\n",
        "        prob_predictions = []\n",
        "        for (name, model), weight in zip(self.models.items(), self.weights):\n",
        "            try:\n",
        "                proba = model.predict_proba(X)\n",
        "                prob_predictions.append(proba * weight)\n",
        "            except:\n",
        "                # If predict_proba fails, skip this model\n",
        "                continue\n",
        "\n",
        "        if prob_predictions:\n",
        "            return np.mean(prob_predictions, axis=0)\n",
        "        return None\n",
        "\n",
        "# Create weighted ensemble with performance-based weights\n",
        "weights = [score for score in top_models.values()]\n",
        "weights = np.array(weights) / np.sum(weights)  # Normalize weights\n",
        "\n",
        "print(f\"Model weights: {dict(zip(top_models.keys(), weights))}\")\n",
        "\n",
        "weighted_ensemble = WeightedEnsemble(\n",
        "    {name: trained_models[name] for name in top_models.keys()},\n",
        "    weights\n",
        ")\n",
        "\n",
        "# Test weighted ensemble\n",
        "weighted_val_pred = weighted_ensemble.predict(X_val_scaled)\n",
        "weighted_val_accuracy = accuracy_score(y_val, weighted_val_pred)\n",
        "print(f\"Weighted Ensemble validation accuracy: {weighted_val_accuracy:.4f}\")\n",
        "\n",
        "if weighted_val_accuracy >= 0.95:\n",
        "    print(f\"TARGET REACHED: {weighted_val_accuracy:.4f} >= 0.95!\")\n",
        "\n",
        "# Add weighted ensemble\n",
        "trained_models['Weighted_Ensemble'] = weighted_ensemble\n",
        "validation_scores['Weighted_Ensemble'] = weighted_val_accuracy\n",
        "\n",
        "# Final memory cleanup\n",
        "gc.collect()\n",
        "check_memory_usage()\n",
        "\n",
        "print(f\"\\nENSEMBLE RESULTS:\")\n",
        "print(\"=\" * 20)\n",
        "ensemble_models = ['Voting_Ensemble', 'Weighted_Ensemble']\n",
        "for name in ensemble_models:\n",
        "    if name in validation_scores:\n",
        "        score = validation_scores[name]\n",
        "        status = \"TARGET REACHED\" if score >= 0.95 else \"CLOSE\" if score >= 0.90 else \"NEEDS IMPROVEMENT\"\n",
        "        print(f\"  {status}: {name:<18} {score:.4f}\")\n",
        "\n",
        "print(f\"\\nMemory-efficient ensemble creation completed!\")\n",
        "print(f\"Available models: {list(trained_models.keys())}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================================================\n",
        "# CELL 16: Final Evaluation on Test Set\n",
        "# ===============================================================================\n",
        "\n",
        "print(\"üéØ FINAL EVALUATION ON TEST SET\")\n",
        "print(\"=\" * 35)\n",
        "\n",
        "def evaluate_model_comprehensive(model, X_test, y_test, model_name):\n",
        "    \"\"\"Comprehensive model evaluation\"\"\"\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        y_test, y_pred, average='weighted', zero_division=0\n",
        "    )\n",
        "\n",
        "    print(f\"\\nüìä {model_name} Test Results:\")\n",
        "    print(f\"   üéØ Accuracy:  {accuracy:.4f} ({'‚úÖ TARGET REACHED!' if accuracy >= 0.95 else 'üìà Close!' if accuracy >= 0.90 else '‚ö†Ô∏è Needs improvement'})\")\n",
        "    print(f\"   üîç Precision: {precision:.4f}\")\n",
        "    print(f\"   üì• Recall:    {recall:.4f}\")\n",
        "    print(f\"   ‚öñÔ∏è F1-Score:  {f1:.4f}\")\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "        'predictions': y_pred\n",
        "    }\n",
        "\n",
        "# Evaluate all models on test set\n",
        "test_results = {}\n",
        "best_accuracy = 0\n",
        "best_model_name = \"\"\n",
        "\n",
        "print(\"üî¨ TESTING ALL MODELS:\")\n",
        "print(\"=\" * 25)\n",
        "\n",
        "for name, model in trained_models.items():\n",
        "    results = evaluate_model_comprehensive(model, X_test_scaled, y_test, name)\n",
        "    test_results[name] = results\n",
        "\n",
        "    if results['accuracy'] > best_accuracy:\n",
        "        best_accuracy = results['accuracy']\n",
        "        best_model_name = name\n",
        "\n",
        "print(f\"\\nüèÜ BEST MODEL: {best_model_name}\")\n",
        "print(f\"üéØ BEST TEST ACCURACY: {best_accuracy:.4f}\")\n",
        "\n",
        "if best_accuracy >= 0.95:\n",
        "    print(\"üéâ CONGRATULATIONS! 95%+ ACCURACY ACHIEVED!\")\n",
        "else:\n",
        "    gap = 0.95 - best_accuracy\n",
        "    print(f\"üìà Gap to 95% target: {gap:.4f} ({gap*100:.2f} percentage points)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVybRr46L8_v",
        "outputId": "8052d6bc-0b9d-4562-8422-3285056d434f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üéØ FINAL EVALUATION ON TEST SET\n",
            "===================================\n",
            "üî¨ TESTING ALL MODELS:\n",
            "=========================\n",
            "\n",
            "üìä Enhanced_RF Test Results:\n",
            "   üéØ Accuracy:  0.8635 (‚ö†Ô∏è Needs improvement)\n",
            "   üîç Precision: 0.8689\n",
            "   üì• Recall:    0.8635\n",
            "   ‚öñÔ∏è F1-Score:  0.8653\n",
            "\n",
            "üìä XGBoost Test Results:\n",
            "   üéØ Accuracy:  0.8768 (‚ö†Ô∏è Needs improvement)\n",
            "   üîç Precision: 0.8825\n",
            "   üì• Recall:    0.8768\n",
            "   ‚öñÔ∏è F1-Score:  0.8783\n",
            "\n",
            "üìä LightGBM Test Results:\n",
            "   üéØ Accuracy:  0.8780 (‚ö†Ô∏è Needs improvement)\n",
            "   üîç Precision: 0.8847\n",
            "   üì• Recall:    0.8780\n",
            "   ‚öñÔ∏è F1-Score:  0.8797\n",
            "\n",
            "üìä Voting_Ensemble Test Results:\n",
            "   üéØ Accuracy:  0.8797 (‚ö†Ô∏è Needs improvement)\n",
            "   üîç Precision: 0.8855\n",
            "   üì• Recall:    0.8797\n",
            "   ‚öñÔ∏è F1-Score:  0.8812\n",
            "\n",
            "üìä Weighted_Ensemble Test Results:\n",
            "   üéØ Accuracy:  0.8491 (‚ö†Ô∏è Needs improvement)\n",
            "   üîç Precision: 0.8543\n",
            "   üì• Recall:    0.8491\n",
            "   ‚öñÔ∏è F1-Score:  0.8508\n",
            "\n",
            "üèÜ BEST MODEL: Voting_Ensemble\n",
            "üéØ BEST TEST ACCURACY: 0.8797\n",
            "üìà Gap to 95% target: 0.0703 (7.03 percentage points)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP81fjotZowWXzvYBQp+ZzU",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}